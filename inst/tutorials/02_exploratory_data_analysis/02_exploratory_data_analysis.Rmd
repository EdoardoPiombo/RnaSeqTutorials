--- 
title: "Exploratory Data Analysis (EDA), _a.k.a._ Biological Quality Assessment"
author: "Nicolas Delhomme for Bn Bioinformatics"
tutorial:
  id: "org.bnbio.tutorials.02_exploratory_data_analysis"
  version: 0.6.2
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, echo=FALSE, eval=TRUE}
# to run locally replace tutorials/02_exploratory_data_analysis/www with inst/tutorials/02_exploratory_data_analysis/www

# to run as an app replace tutorials/02_exploratory_data_analysis/www with www

# libs
library(DESeq2)
library(here)
library(learnr)
library(tidyverse)
library(vsn)

# options
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(
  exercise.reveal_solution=TRUE)

# helpers
source(here("inst/tutorials/02_exploratory_data_analysis/www/featureSelection.R"))

# data

## Edoardo
## Substitute here with a group-specific data file.
## We can maybe give each group a pathway for the files and use a learnr question to have them add it?

# sample_file <- here("inst/tutorials/02_exploratory_data_analysis/www/samples_table.tsv")
# samples <- read_tsv(sample_file,col_types=c("cfff"))

# step to reproduce
# ---
# txi <- readRDS(here("inst/tutorials/02_exploratory_data_analysis/www/tximport.rds"))
# counts <- round(txi$counts)
# saveRDS(counts,file=here("inst/tutorials/02_exploratory_data_analysis/www/counts.rds"))
# dds <- DESeqDataSetFromTximport(
#   txi=txi,
#   colData = samples,
#   design = ~ time * treatment)
# dds <- estimateSizeFactors(dds)
# saveRDS(dds,file=here("inst/tutorials/02_exploratory_data_analysis/www/dds.rds"))
# vsd <- varianceStabilizingTransformation(dds, blind=TRUE)
# vst <- assay(vsd)
# vst <- vst - min(vst)
# saveRDS(vst,file=here("inst/tutorials/02_exploratory_data_analysis/www/vst.rds"))
# pc <- prcomp(t(vst))
# percent <- round(summary(pc)$importance[2,]*100)
# conds <- factor(paste(dds$time,dds$treatment))
# sels <- rangeFeatureSelect(counts=vst,conditions=conds,plot=FALSE)
```

## Introduction

In this tutorial, we will take a look at the RNAseq data that you generated last week.


For this tutorial, you will work with a number of samples deriving from two conditions.

Explore the metadata file and consider which comparison is could you do with your data.

<!-- Edoardo
This question was a quiz originally, but since the answer would change with every group I though of just leaving an open question.-->

  **Notes:**

  1. the library `here` is an essential library to ensure reproducibility. You might be familiar with _base_ `R` `setwd()` (set working directory) function? The `here()` function is similar but unlike `setwd()` that defines the directory according to your computer (_e.g._ `/home/delhomme/some-project`), it dynamically defines that directory based on the computer it is on. So on someone else's computer it will resolve to _e.g._ `/home/bastian/some-project`. It might seem trivial, but it makes the code truly portable, hence reproducible out of the box.
  2. the function `read_csv` is only one of many functions from the `readr` package, a part of tidyverse. Check its [cheatsheet](https://readr.tidyverse.org/index.html#cheatsheet). You can also read in data directly from a Google Sheet!
  3. [chapter 11](https://r4ds.had.co.nz/data-import.html#getting-started) explains in great details the mechanism of how `readr` parses the data and how you can fine tune it to your needs. While this is relatively infrequent in practice, it is good to know where to look for, whenever needed.

```{r which-comparison, exercise=TRUE, exercise.eval=TRUE}
library(here)
library(readr)

#Specify the path to your metadata file
metadata_file <- 'your_metadata_file.csv'

#If you do not set your own metadata file, a default one is used:
# Check if 'metadata_file' is defined
if (!exists("metadata_file")) {
  # If 'metadata_file' is not defined, define it
  metadata_file <- here('inst/tutorials/02_exploratory_data_analysis/02_exploratory_data_analysis_files/pine_cold_root_metadata.csv')
} else if (!file.exists(here(metadata_file))) {
  # If the file doesn't exist, use a default file
  metadata_file <- here('inst/tutorials/02_exploratory_data_analysis/02_exploratory_data_analysis_files/pine_cold_root_metadata.csv')
}

metadata <- read_csv(metadata_file)


```

Now that we know more about the study design, we will go through the following steps (this tutorial is focused on the first one solely):

1. Perform an assessment of the data; the so called exploratory data analysis. It will help us decide whether the data is of sufficient quality to answer out biological question.
2. Once satisfied with 1., perform a gene differential expression analysis to identify candidate genes
3. Gain more knowledge about the identified candidate genes to generate further hypotheses and/or select candidates for follow up studies.

As mentioned, this tutorial focuses on point 1, which includes the following steps

1.
  1. gather the data and its metadata, and import it into `R` learning more about useful functions to do so, as well as about the properties of _RNA-Seq_ data.
  2. perform an exploratory data analysis (EDA) of the raw data (as we did in the previous tutorial)
  3. normalize the data, so as to make it comparable between samples, further highlighting _RNA-Seq_ data properties
  4. perform an EDA on the normalised data
  5. conclude whether we can move onto point 2. or if countermeasures need to be taken to ensure our question can be answered

## Metadata

We have translated the information from the study abstract into a file called [samples_table.tsv](tutorials/02_exploratory_data_analysis/www/samples_table.tsv), which looks like this:

```{r metadata,echo=FALSE, eval=TRUE}
## They should have already loaded the metadata at this point
if (!exists("metadata_file")) {
  # If 'metadata_file' is not defined, define it
  metadata_file <- here('inst/tutorials/02_exploratory_data_analysis/02_exploratory_data_analysis_files/pine_cold_root_metadata.csv')
} else if (!file.exists(here(metadata_file))) {
  # If the file doesn't exist, use a default file
  metadata_file <- here('inst/tutorials/02_exploratory_data_analysis/02_exploratory_data_analysis_files/pine_cold_root_metadata.csv')
}
head(metadata)
```


---

There is a lot more options that can be used to tune importing the data. The `R` language has its origin has a statistical language. Many of the variables we have in our metadata are actually categorical variables, which are best represented in `R` as `factor`s. Re-read the metadata setting the relevant columns as factors.

```{r metadata2, exercise=TRUE, exercise.eval=TRUE, message=FALSE, exercise.setup="metadata"}

metadata <- read_csv(metadata_file)
```

```{r metadata2-hint}
read_csv(...,col_types=...)
```

```{r metadata2-solution, eval=TRUE}
# multiple solutions are possible
# using a string
samples <- read_csv(metadata_file,col_types=c("cfff"))
samples
# setting a default overall
samples <- read_csv(metadata_file,col_types=cols(.default=col_factor()))
samples
# or you can have an extensive control of all variables.
# if you replace cols() with cols_only(), you can control the columns that are kept
samples <- read_csv(metadata_file,col_types=cols(sample_id=col_character(),
                                               replicate=col_factor(),
                                               time=col_factor(),
                                               treatment=col_factor()))
samples
# sometimes you just want the defaults, but also want it to be quiet 
# (not a solution to the question though! but as good as any a place to mention it)
samples <- read_csv(metadata_file,show_col_types=FALSE)
```

Alright, half of the work done, remains to load the data.

## Count data

RNA-Seq data measures mRNA fragments, so it is aimed at quantifying the expression of genes, _i.e._ the first step of the central dogma `DNA->RNA->protein`. But as we know, biology is much more complex (_"Biology is too important to be left to the biologists"_ - attributed to _Niels Bohr_ to his then student _Max Delbrück_ - yes the founder of molecular biology as we know it).

One advantage is that it also captures, in higher eukaryotes, splicing variants.

![[Wikipedia](https://en.wikipedia.org/wiki/Alternative_splicing)](images/DNA_alternative_splicing.gif){width=80%}

There are several caveats too, which we need to keep in mind.

```{r caveats}
quiz(caption="RNA-Seq caveats",
     question("Which of the following are likely caveats of bulk RNA-Seq",
              answer("averages a population of 10^6 cells"),
              answer("consists of a mix of cells of different origins"),
              answer("cells sampled at a very exact timepoint",message="biological processes stop the exact moment you freeze the sample"),
              answer("all of the above and more - keep a critical mind!",correct=TRUE)
     ),
     question("What of single cell RNA-Seq?",
              answer("very low material, some transcript are present in less than a copy/cell"),
              answer("sequencing depth is limited, large drop-out effect"),
              answer("signal can be drown out by strong biological processes (_e.g._ cell cycle)"),
              answer("all of the above and more - keep a critical mind!",correct=TRUE)
     ))
```

Nonetheless, RNA-Seq is the best tool we have for now (!) for doing expression quantification. But it is **at best** a proxy and only one layer of gene regulation (mRNA half-life, mRNA storage, post-transcriptional modification, editing, _etc._).

---

On the brighter side, what can we do with count data? A lot!

1. Look whether the total output of a gene changes between conditions: Differential Gene Expression (DGE)
2. The same at the transcript (splicing isoform) level: Differential Transcript Expression (DTE)
3. Assess whether any isoform of a given gene changes: DTE+G
4. Does the isoform composition of a given gene change: Differential Transcript Usage (DTU)
5. Does anything change? [Gene Differential Expression (GDE)](https://liorpachter.wordpress.com/2018/02/15/gde%C2%B2-dge%C2%B2-dtu%C2%B2-dte%E2%82%81%C2%B2-dte%E2%82%82%C2%B2/)

![By Lior Pachter](images/GeneDifferentialExpression.jpeg){width=80%}
We will only work with point 1 (Differential Gene Expression) during this course

---

### Counts properties

#### Relative

Counts have many properties, which we will see in the following, but most importantly, they are **relative** abundances. Indeed, they
1. originate from transcripts of different length
2. are part of sequenced pools of different size (sequencing depth)

---

#### Units

An important note on units. You may have heard of `RPKM`, `FPKM`, `TPM`. These are all metrics used for RNA-Seq and they have been used for years. Tools for differential expression do **NOT** use any of these and for good reasons (more below). 

It is not important for you to know exactly what these are (but you can google them of course!). Just know that all three units will be affected by genes with very high counts, which have high variance and disproportionately influence estimators like the total sum. They merely normalise all expression values using a single ratio, scaling to a final library size equivalent to one million

`RPKM/FPKM` (read/fragment per kilobase of transcript per million reads in the library) are even more sensitive to this than the `TPM` as they integrate the actual library size as part of the calculation instead of that of an arbitrary library size of one million.

[Here](https://docs.google.com/document/d/1D5CoNPxy45MpXLLvbIImFzCebF-fe0jD5KLeAQibGTI) is a document we have used for years to convince authors and editors that these measures are flawed. This has now become mainstream with many studies highlighting the risks of using such measures for differential expression. 

`TPM` are still being reported by many tools and using them for summary statistics and visualisation at the sample level is broadly accepted.

---

#### Statistical consideration

Counts have the following properties:

1. variance depends on the mean count
2. counts are non-negative and highly skewed

<br/>
<br/>

![By Q9 at the English-language Wikipedia, CC BY-SA 3.0, [Wikipedia](https://commons.wikimedia.org/w/index.php?curid=18064846)](images/Heteroscedasticity.png)

---

<!-- Edoardo
Do the students possess the background to understand the next part? It is not striclty necessary for them to know this -->

Such characteristics are well modeled by a Poisson distribution with its single parameter lambda: 

$\lambda = mean = variance$

$$\vartheta^{2} = \mu$$

![By Skbkekas - Own work, CC BY 3.0, [Wikipedia](https://commons.wikimedia.org/w/index.php?curid=9447142)](images/Poisson_pmf.svg)

The Poisson distribution represents well technical replicates (where mRNA proportions are identical across samples), but cannot account for the over-dispersion observed across biological replicates. We need a distribution that can account for that, _i.e._ that allows for the mRNA proportions to vary across samples. That generalisation is the **negative binomial** distribution. 

$$\vartheta^{2} = \mu + \theta\mu^{2} $$
where $\theta$ is the dispersion and $\sqrt{\theta}$ is the "biological coefficient of variation".

---

_Fun fact_:
  The negative binomial is the best model for tornadoes in the US tornado alley, _c.f._ [Nouri et _al._, 2021](https://www.nature.com/articles/s41598-021-81143-5). 

_trivia_: 
  It is a 100% `ggplot2` manuscript...

---

More seriously, the negative binomial is the best model we have for the time being for RNA-Seq data - **All models are wrong but some are useful - George E. P. Box**.

As a matter of fact, differential expression models that have more than one variable are extremely computationally expensive to compute using a negative binomial model, hence they rely on _generalized linear model_ instead.

### `tximport`

Quantification of expression profiling data from most commonly used tools (_e.g._ salmon, rsem, stringtie, kallisto) can be imported in `R` using the `tximport` [package](https://doi.org/doi:10.18129/B9.bioc.tximport).

<!-- Edoardo
I think we could leave out the part about the matrix of transcript lengths
-->

_"Imports transcript-level abundance, estimated counts and transcript lengths, and summarizes into matrices for use with downstream gene-level analysis packages. Average transcript length, weighted by sample-specific transcript abundance estimates, is provided as a matrix which can be used as an offset for different expression of gene-level counts."_ [Soneson, Love, Robinson](https://doi.org/10.12688/f1000research.7563.1)

Use tximport to load your data. Remember to use the tx2gene file specified for your group on Canvas. This file is necessary if your species has more than one transcript per gene. It should then contain two columns, tab delimited, the first one with the transcript IDs and the second one the corresponding gene ID.

```{r txi_import, exercise= TRUE, exercise.eval=TRUE, exercise.setup = "metadata2-solution"}
library(tximport)

#Specify the path to the tx2gene of the organism used in your group, you find them on Canvas
tx2gene_file <- 'your_tx2gene_file.csv'

#If you do not set your own tx2gene file, a default one is used:
if (!file.exists(here(tx2gene_file))) {
  # If the file doesn't exist, use a default file
  tx2gene_file <- here('inst/tutorials/02_exploratory_data_analysis/02_exploratory_data_analysis_files/tx2gene_Pinsy01_coding_genes.tsv')
}


tx2gene <- read_delim(here(tx2gene_file),delim="\t",
                                 col_names=c("TXID","GENE"))


salmon_folder <- "YOUR_SALMON_FOLDER"
 
#If you do not set your own salmon folder, a default one is used:
if (!file.exists(here(salmon_folder))) {
  # If the file doesn't exist, use a default file
  salmon_folder <- here('inst/tutorials/02_exploratory_data_analysis/02_exploratory_data_analysis_files/salmon')
}


filelist <- list.files(here(salmon_folder), 
                          recursive = TRUE, 
                          pattern = "quant.sf",
                          full.names = TRUE)

#' This step is to validate that the salmon files are int he same order as 
#' described in the metadata file. This is fundamental, as the metadata and the data come from separate files, and if the order is different the next steps will "swap" samples (that is, they will not be able to attribute the right condition to the right sample).
#' If they are not in the same order, you must change the order using R,
#' or by editing the metadata file directly and then rerunning the whole BiologicalQA.
#' Similarly, people working with aspen do not need to do this.
#' Instead of TO_REPLACE and REPLACEMENT, you should add a regular expression to have your complete file names match your Sample IDs
#' You can complete regular expression is in the text in the REGEX column corresponding to your group
#' in the regex.tsv document (On Canvas).
#' Regular expressions are strings of text that allow a program to match different string of texts. For example, "." means
#' "any character" and "*" means "any number of repetition of the previous character
#' Can you understand why your regex work?
#' If you feel brave, you can of course try to come up with a regex of your own.
#' Pine projects have an expecially simple regular expressions
#' 
stopifnot(all(match(sub("_S[0-9]+_L00[0-9]_sortmerna_trimmomatic.*","",basename(dirname(filelist))),
                    samples$SampleID) == 1:nrow(samples)))
samples <- samples %>% mutate(Filenames = filelist)
txi <- tximport(files = samples$Filenames,
                                 type = "salmon",
                                 tx2gene=tx2gene)

colnames(txi$counts) <- samples$SampleID

#Let's save the counts to a separate file, in case we need them later:

counts_file <- "PATH_TO_PROJECT_FOLDER/counts.csv"

#If you do not set your own counts file, a default one is used:
if (!file.exists(here(counts_file))) {
  # If the file doesn't exist, use a default file
  counts_file <- here('inst/tutorials/02_exploratory_data_analysis/counts.csv')
}


write.csv(as.data.frame(txi$counts), file = counts_file)

# Let's also save the samples, in the right order and with a column indicating their salmon quant files, as a new file. You must uncomment the following line for this to work:

#write_tsv(samples,"PATH_TO_PROJECT_FOLDER/samples_full_rank.txt")

```


<!-- Edoardo
I commented this part as I think it will cause confusion. They did not go this deep on salmon during week1 and I would not want them to spend too much time in week2 googling how salmon works.

Note that salmon was used in a mode that bootstraps the quantification (since it is an expectation-maximisation process) typically a 100 times. This allows to devise per-sample inferential replicate information, which was then summarised by setting `varReduce=TRUE`. The `txi` object was saved and we now load it into our session.
-->

Take a look at the object (use the code block above). Make sure *NOT* to call `txi` as this will most probably time-out.

<!-- Edoardo
In the next part the answers will change depending on the group, so I shifted to open ended questions
-->

```{r tximport-quiz}
quiz(caption="`tximport` object",
     question("What is it?",
              answer("a list",correct=TRUE),
              answer("a data.frame"),
              answer("an S4 object")),
     textInput("num_values", "How many values are stored in it?"),
     textInput("num_genes", "How many samples and how many genes were reported?"),
     question("Looking at the value names, what is the difference between 'abundance' and 'counts'?",
              answer("Both are counts",correct=TRUE),
              answer("The first is reported in TPM",correct=TRUE),
              answer("The second is the raw estimates reported by salmon",correct=TRUE),
              incorrect="All are true, as `countsFromAbundance` is `no`"
              ),
     question("What about the 'length', what does it contain?",
              answer("The expected length of the transcripts."),
              answer("The observed length of the transcripts",correct=TRUE),
              answer("The length of the transcripts normalised for the read length")
              ),
     question("What about 'variance'?",
              answer("the summarised per-sample inferential replicate information",correct=TRUE),
              answer("the variance of the 'length'"),
              answer("the variance of the 'abundance'"),
              incorrect="Reread what is just above that quiz :-)"
              )
     )
```

### `tximeta`

We have now seen `tximport` but very often you would want to get more than the gene ID and might like to have the gene name, symbol, description, _etc._ associated with the count information already. This is possible using the [`tximeta`](http://bioconductor.org/packages/release/bioc/vignettes/tximeta/inst/doc/tximeta.html) package. We will not look at it as part of this tutorial, so this is for your reference only. We will later in the course learn how to retrieve such information and associate it with our results.

---

Now that we have a good idea of our data and metadata, let us look at them!

## Exploratory Data Analysis

We will start by extracting the counts that we need. 

```{r count-quiz}
quiz(caption="raw or TPM, that is the question",
     question("Which of 'abundance' or 'counts' should be retrieved?",
              answer("counts, provided 'countsFromAbundance=no'",correct=TRUE),
              answer("counts",message="That is only a partially correct answer"),
              answer("abundance",message="rather not, these are TPMs. Unless you know what you are doing and aware of the possible caveats.")
     ))
```

Write the code to extract the counts and apply a function to convert them to integers.
Integers is a fancy coding name for "numbers without decimals". Sometimes you can have decimal numbers in counts, for example because some programs assign "0.5" value to two transcripts when a read maps equally well to both of them. We must round all values so we do not have decimals, otherwise some of the programs of the following steps will not work.



```{r counts, exercise=TRUE, exercise.eval=FALSE, exercise.setup="txi_import"}

```

```{r counts-hint}
?round
?`$`
```

```{r counts-solution}
counts <- round(txi$counts)
```

<!-- Edoardo: I commented the following part, as they should have already done that at this point

Now, it is very important that we do some sanity checking. Our metadata and data are two **SEPARATE** entities. We need to ensure that they are from the same studies, that samples are the same and most importantly sorted in the same order! Give it a try.
```{r counts-pre, include=FALSE}
counts <- readRDS(here("inst/tutorials/02_exploratory_data_analysis/www/counts.rds"))
```

```{r sanity, exercise=TRUE, exercise.eval=FALSE, exercise.setup="counts-pre"}

```

```{r sanity-hint}
?colnames
?`==`
?all
?stopifnot
```

```{r sanity-solution}
stopifnot(all(colnames(counts) ==  samples$sample_id))
```

Hurrah! Our metadata and data reference the same samples, in the same order!

```{r sanity-quizz}
quiz(caption="Does it matter?",
     question("Does it?",
              answer("Sure! You did not swap the samples in the wet lab, why do it in the dry one?",
                     correct=TRUE),
              answer("No, it does not, R or whatever package will fix it for me",
                     message="That would be nice")
     ))
```
-->


### Raw data

#### Never-observed-events proportion

One of the first question is to figure out how many of the genes are never observed in any of the samples. Write the code to get that as a percentage of the total number of genes


```{r noexp, exercise=TRUE, exercise.eval=FALSE}

```

```{r noexp-hint}
?rowSums
?`==`
?nrow
```

```{r noexp-solution}
sel <- rowSums(counts) == 0
sprintf("%s%% percent (%s) of %s genes are not expressed",
        round(sum(sel) * 100/ nrow(counts),digits=1),
        sum(sel),
        nrow(counts))
```

```{r noexp-quiz}
quiz(caption="Assumption",
     question("Was that an amount you expected?",
              answer("Yes",correct=TRUE),
              answer("No",message="This is actually also a valid answer"),
              post_message="Whichever your answer, keep your arguments so we can discuss them."
     ))
```

#### Sequencing depth (SD)
Another important initial assessment is to evaluate the discrepancy (if any) in sequencing depth. For that, visualise the sum of counts per samples. For this, you will use `ggplot` but you will also need to create the necessary data. For that we will need the `tibble` package `tibble()` constructor and the `dplyr` package `%>%` pipe and `bind_cols()` function. These are all packages included in tidyverse, and both the tidyverse [webpage](https://www.tidyverse.org/) and the corresponding [cheatsheets](https://www.rstudio.com/resources/cheatsheets/) are extremely useful.

First, write the code to reproduce the following object:
```{r tibble-view, echo=FALSE, eval=TRUE}
counts <- round(txi$counts)
tibble(x=colnames(counts),
       y=colSums(counts)) %>% 
  bind_cols(samples)
```

```{r tibble,  exercise=TRUE, exercise.eval=FALSE, exercise.setup="counts-solution"}

```

```{r tibble-hint}
?tibble
?`%>%`
?bind_cols
```

```{r tibble-solution}
dat <- tibble(TotalCounts=colSums(counts)) %>% 
  bind_cols(samples)
```

Now that we have the data in the correct shape, visualise it using ggplot.

```{r barplot, exercise=TRUE, exercise.eval=FALSE, exercise.lines = 10, exercise.setup="tibble"}
dat <- tibble(TotalCounts=colSums(counts)) %>% 
  bind_cols(samples)

```

```{r barplot-hint-1}
?geom_col
?facet_grid
```

```{r barplot-hint-2}
#In the following code, you should substitute "Condition" with the name of the column containing the condition changing in your samples.
ggplot(dat,aes(x=,y=,fill=)) + 
  geom_col() + 
  scale_y_continuous(name=) +
  facet_grid(~ factor(), scales = "free") +
  theme_bw() + 
  theme(axis.text.x=element_text(angle=90,size=6),
        axis.title.x=element_blank())
```

```{r barplot-solution}
#In the following code, you should substitute "Condition" with the name of the column containing the condition changing in your samples.
ggplot(dat,aes(x=SampleID,y=TotalCounts,fill=Condition)) + 
  geom_col() + 
  scale_y_continuous(name="reads") +
  facet_grid(~ factor(Condition), scales = "free") +
  theme_bw() + 
  theme(axis.text.x=element_text(angle=90,size=6),
        axis.title.x=element_blank())
```

```{r barplot-quiz}
quiz(caption="Assumption",
     question("Is there something unexpected in the plot?",
              answer("Yes",correct=TRUE),
              answer("No",message="This could actually also a be valid answer, depending on the group"),
              post_message="Whichever your answer, keep your arguments so we can discuss them."
     ))
```

#### SD per sample
As we just saw, some samples have more sequencing depth than others. That previous visualisation however does not inform us on the distribution of reads within individual samples. To achieve this we will again need to transform the data. We will convert the counts onto a log10 scale and store them in a `data.frame()`. Then we will use the `tibble` package `rownames_to_column()`, `tidyr` `pivot_longer` and `dplyr` `starts_with()` and `left_join()` functions to convert the data and associate it with the sample information. Write the code to reproduce the following table.

```{r pivot-view, echo=FALSE, eval=TRUE}
#In the following code, you should substitute "P21452" with the start of the name of your samples.

as.data.frame(log10(counts)) %>% 
  rownames_to_column() %>% 
  pivot_longer(starts_with("P21452"),
               names_to="SampleID",
               values_to="x") %>% 
  left_join(samples,by=c("SampleID"))
```

```{r pivot, exercise=TRUE, exercise.eval=FALSE, exercise.setup="barplot"}

```

```{r pivot-hint-1}
?as.data.frame
?log10
?rownames_to_column
?pivot_longer
?starts_with
?left_join
```

```{r pivot-hint-2}
dat <- as.data.frame(LOG_DATA) %>% 
  rownames_to_column() %>% 
  pivot_longer(starts_with(COUNT_DATA_COLUMNS_PREFIX),
               names_to="sample_id",
               values_to="x") %>% 
  left_join(samples %>% select(-replicate),by=c("sample_id"))
```

```{r pivot-solution}
dat <- as.data.frame(log10(counts)) %>% 
  rownames_to_column() %>% 
  pivot_longer(starts_with("P21452"),
               names_to="SampleID",
               values_to="x") %>% 
  left_join(samples,by=c("SampleID"))
```

Now that we have the data in the right shape, we can visualise it. For that we can use the density geom, and as while we want to visualise the individual samples, we need to have a corresponding `group` aesthetic.

```{r density, exercise=TRUE, exercise.eval=FALSE, exercise.lines=18, exercise.setup="pivot"}
dat <- as.data.frame(log10(counts)) %>% 
  rownames_to_column() %>% 
  pivot_longer(starts_with("P21452"),
               names_to="SampleID",
               values_to="x") %>% 
  left_join(samples,by=c("SampleID"))
```

```{r density-hint-1}
?geom_density
?aes
```

```{r density-hint-2}
ggplot(dat,aes(x=x,group=GROUP,col=COLOR)) + 
  geom_density(na.rm = TRUE) + 
  ggtitle("sample raw counts distribution") +
  scale_x_continuous(name="per gene raw counts (log10)") + 
  theme_bw()
```

```{r density-solution}
ggplot(dat,aes(x=x,group=SampleID,col=Condition)) + 
  geom_density(na.rm = TRUE) + 
  ggtitle("sample raw counts distribution") +
  scale_x_continuous(name="per gene raw counts (log10)")

```

Do all your samples have the same distribution? If not, what would that mean? 

```{r density-quizz}
quiz(caption="What's next",
     question("What would you do with a sample having a different distribution?",
              answer("Proceed with it",correct=TRUE),
              answer("Drop it",message="On what justification? So far we only know it is not sequenced as deep as the others."),
              answer("Ask the wet-lab or sequencing facility if there is still some material available for a sequencing re-run.", correct=TRUE)))
```

### Normalisation

The property of the raw data looks acceptable, with the drawback that some samples may have different sequencing depth (depending on the group). We will now normalise the data so as to be able to compare between samples.

#### Principles
We know we need to address the following caveats:

1. Counts are relative to their sample
2. Sequencing depth is variable across samples
3. Counts variance depends on the mean count (heteroscedastic)
4. Counts are non-negative and highly skewed (data follows a negative binomial distribution)

---

How do we address 1. and 2.?

A naive approach such as that used by TPM and RPKM/FPKM is flawed. So calculating a normalisation factor using _e.g._ 
$$\sum_{c_{i}}/\sum_{c_{ref}}$$
where $c_{i}$ are the counts from one sample and $c_{ref}$ the counts from a reference sample (one pseudo average sample or one selected sample) would result in the following:

![Charlotte Soneson, Cambridge RNA-Seq course, 2019](images/rna_composition_1.png){width=80%}

Clearly, this would fail at correcting for the influence of the sample RNA composition.

What if we make the assumption that **most genes are not differentially expressed** (**THIS** is the most critical assumption we do for Differential Expression!)?

$$\sum_{c_{i}l_{i}}/\sum_{c_{ref}}$$
where $l_{i}$ is a weight for compensating for the  difference in library composition.

![Charlotte Soneson, Cambridge RNA-Seq course, 2019](images/rna_composition_2.png){width=80%}

That is much more like it! This is what packages like `edgeR` and `DESeq2` will do for you. For example, `edgeR` will apply method called "TMM", the `trimmed mean of medians` to calculate that normalisation factor, _a.k.a_ size factor, _i.e._ the ratio correcting for the difference in sequencing depth.

---

#### Setup

Now that we know the theory, put it in practice using the `DESeq2` package. First, create a `DESeqDataSet` object from your `txi` and `samples` objects. It will require that we set a `design`. For this, recall the very beginning of this tutorial and that we agreed on the fact that one of your variables is of interest (depending on the group). The design is what tells `DESeq2` how we want to model the expression values.

<!-- Edoardo: I commented out the next part, as for them it will be easier to just now to use the
name of the variable of interest as design


$$expression = f(treatment + time + treatment:time)$$
We will revisit this concept in the next tutorial, for now, just know that in `R`, we write the previous equation as `~ time * treatment`

-->

Before actually loading the data, we set as reference level the condition that we will compare the data to.
If you have it in your dataset, this could be the "control" condition, otherwise it should be one of the values present in the column of the metadata you are interested in.
When a gene is identified as "Upregulated" or "Downregulated", it will be upregulated or downregulated in respect to this condition.
Now try to load the data using DESeq2 commands

```{r dds, exercise=TRUE, exercise.eval=FALSE, exercise.setup="txi_import"}
suppressPackageStartupMessages(library(DESeq2))

#Here and in the following code, you must substitute "Condition" with your condition of interest
#Moreover, you must replace "24h_neg5C" with the value of your condition of interest corresponding to the control.
#This is the value you will compare expression to.
samples$Condition <- as.factor(samples$Condition)
samples$Condition <- relevel(samples$Condition, ref = "24h_neg5C")

dds <- 
```

_Note:_ `suppressPackageStartupMessages()` is very useful to silence all messages and warnings that `R` rpoduces when loading a package. However, one should be careful as these messages can point to issues such as the overloading of functionalities of packages previously loaded.

```{r dds-hint}
?DESeqDataSetFromTximport
```

```{r dds-solution}
samples$Condition <- as.factor(samples$Condition)
samples$Condition <- relevel(samples$Condition, ref = "24h_neg5C")

dds <- DESeqDataSetFromTximport(
  txi=txi,
  colData = samples,
  design = ~ Condition)
```

 **Note:**
See the warning we got? Well, up there reading the metadata, the last command reset samples to have all columns as characters. This is `R` default since version 4, but in previous versions, factors were the default to represent alphanumeric values.


Before we proceed, take a few instants to familiarise with the `DESeqDataSet` object.
Then, save the dds object like an rda file (a file format used to save this type of objects), so that we can import it again tomorrow.
```{r dds-what, exercise=TRUE, exercise.eval=TRUE, exercise.setup="dds-solution"}
dds
colData(dds)
colnames(dds)
head(rownames(dds))
head(assay(dds))
head(assay(dds,"avgTxLength"))
metadata(dds)

deseq2_file <- "PATH_TO_PROJECT_FOLDER/dds.rda"
#If you do not set your own deseq2 file, a default one is used:
if (!file.exists(here(deseq2_file))) {
  # If the file doesn't exist, use a default file
  deseq2_file <- here('inst/tutorials/02_exploratory_data_analysis/dds.rda')
}

save(dds,file=deseq2_file)
```

If you observe the example above, you will realise that all the function calls past the first view of the `dds` object are using `accessor` functions that are named after the `slot` of the object. This is part of the good coding practice in `R` and something you should come to expect when working with `S4-6` objects.

---

#### Size factors

Now that we have the object ready, we can estimate the normalisation factor for the difference in sequencing depth using `DESeq2` `estimateSizeFactors()` function. Write the code to do so.

```{r size_factors, exercise=TRUE, exercise.eval=FALSE, exercise.setup="dds-solution"}

```

```{r size_factors-hint}
?estimateSizeFactors
```

```{r size_factors-solution}
dds <- estimateSizeFactors(dds)
```

As you may observe, `DESeq2` informs us that it will use the average transcript length to correct the expression values. The size factor will be calculated per gene per library, so the `DESeq2` function `normalizationFactor()` will return a matrix. Our next assessment is to figure out how (dis)similar these factors are across our dataset. Revisit the code we wrote for visualising the raw data count distribution per sample to visualise the normalisation factor. Instead of a `geom_bar()`, try to use a `geom_violin()`

```{r norm_factor, exercise=TRUE, exercise.eval=FALSE, exercise.setup="dds-solution"}

```

```{r norm_factor-hint}
?as.data.frame
?normalizationFactors
?rownames_to_column
?pivot_longer
?starts_with
?geom_violin
```

```{r norm_factor-solution}
#Here you must enter the first few letters appearing in every sample name, instead of "P21452"
ggplot(as.data.frame(normalizationFactors(dds)) %>% 
          rownames_to_column() %>% pivot_longer(starts_with("P21452"),
                                                names_to="SampleID",
                                                values_to="size_factor"),
        aes(x=SampleID,y=size_factor)) + 
  geom_violin() + 
  ylim(c(0,2)) + 
  theme(axis.text.x=element_text(angle=90,size=6),
        axis.title.x=element_blank())
```

---

#### Heteroscedasticity

Now that we have seen how the library size normalisation works, and that most libraries have a very narrow, hence, highly specific normalisation factor across genes per sample (which is a first indication that our major assumption holds!), we can look into the 3. and 4. properties of the data and how we can normalise that. The issues with these points are:

3. All parametric statistics rely on the assumption that the variance is independent of the mean.
4. errors are multiplicative on a linear scale

---

**Important note** DESeq2 and edgeR only models the parameters, they will **always** (!) use the raw counts in their calculation. There is a possibility to extract the normalised counts from both packages, _e.g._ `DESeq2` `counts(dds,normalised=TRUE)` function call.

---

#### Variance Stabilising Transformation

To address both issues 3. and 4. (that are by the way not corrected when you export using `DESeq2` `counts(dds,normalised=TRUE)` function call), we will use another `DESeq2` function called `varianceStabilizingTransformation()`
As for dds, we save the vst object in rda format

```{r vst, exercise=TRUE, exercise.eval=TRUE, exercise.setup="dds-solution"}
vsd <- varianceStabilizingTransformation(dds, blind=TRUE)
vst <- assay(vsd)
vst <- vst - min(vst)

vst_file <- "PATH_TO_PROJECT_FOLDER/vst-aware.rda"
#If you do not set your own vst file, a default one is used:
if (!file.exists(here(vst_file))) {
  # If the file doesn't exist, use a default file
  vst_file <- here('inst/tutorials/02_exploratory_data_analysis/vst-aware.rda')
}

save(vst,file=vst_file)
```

_Note_
1. `vst()` is also a function in `DESeq2`. It does the same as `varianceStabilizingTransformation()` but on a smaller random subset of the data. So it is much faster, but possibly less accurate. This might be useful for EDA on large dataset, but not it you want to do any statistical tests on that normalised data.
2. The `blind` argument controls whether the design should be used as a prior to the transformation, _i.e._ if the variance should be estimated across all samples (`TRUE`) or per replicates as defined by the design. You would want the former when you are performing an exploratory analysis, so as not to bias your results, while you would want to set it to `FALSE` when you are satisfied that your design is the best model for your data, so as to increase the transformation results' accuracy.

We can look now at what the `vst` has achieved.



```{r vst-rds, include=FALSE}
counts <- as.matrix(read.csv(here(counts_file), row.names = 1))
load(here(deseq2_file))
load(here(vst_file))
dds <- estimateSizeFactors(dds)
```


```{r htrscdstc, exercise=TRUE, exercise.eval=FALSE, exercise.setup="vst-rds"}
library(vsn)
meanSdPlot(counts[rowSums(counts)>0,])
meanSdPlot(counts(dds)[rowSums(counts(dds))>0,])
meanSdPlot(log1p(counts(dds))[rowSums(counts(dds))>0,])
meanSdPlot(log1p(counts(dds,normalized=TRUE))[rowSums(counts(dds))>0,])
meanSdPlot(log1p(counts(dds,normalized=TRUE))[rowSums(counts(dds,normalized=TRUE))>0,])
meanSdPlot(vst[rowSums(vst)>0,])
```

### Visualisation

Now, we will focus on a number of visualisations to perform the EDA of the normalised data. The most common visualisations are Principal Component Analysis ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) and [heat maps](https://en.wikipedia.org/wiki/Heat_map).

#### PCA

The first step is to actually perform the PCA before visualising it. What we are interested in is to assess how similar samples are to each other. For that, we can use the _base_ `R` `prcomp` function. That function calculates the PCA by rows, so we will need to transpose our normalised data `t()` prior to computing the PCA. There is no need to re-invent that wheel, so just run the code below :-)

```{r prcomp, exercise=TRUE, exercise.eval=TRUE, exercise.setup="vst-rds"}
pc <- prcomp(t(vst))
(percent <- round(summary(pc)$importance[2,]*100))
```

_Note_ Adding a pair of parenthesis `()` around an assignment function is a neat trick in `R` to both perform the assignment and print the result of the function call in the console.

```{r prcomp-quiz}
quiz(caption="What does a PCA tells us?",
     question("What are the numbers reported",
              answer("The percentage of variance explained per components",correct=TRUE),
              answer("The number of genes (data points) whose variance best defines the components")))
```

We will now look at a scree plot and a 2D plot of the components. For you reference, there exists a fantastic package to analyses PCA plots: [PCAtools](https://bioconductor.org/packages/devel/bioc/vignettes/PCAtools/inst/doc/PCAtools.html)

---


A scree plot is a plot that visualise the cumulative variance explained by the components. It is useful to determine how many components are needed to explain most of the variance in the data, it will help us assess whether that number of components could be in agreement with the number of variables as well as with the number of possible combinations of their values.

Again, here we can ignore the code and focus on the interpretation of the scree plot.

```{r scree, exercise=TRUE, exercise.eval=FALSE, exercise.setup="prcomp"}
# variables in the model
vars <- all.vars(design(dds))
# number of variables
nvar <- length(vars)
# dynamic programming to find out the number of combinations of the variables
nlevel<-reduce(sapply(vars,function(v){nlevels(eval(parse(text=paste0("dds$",v))))}),`*`)

# plotting the cumulative variance explained
# adding red lines for the numbers of variables
# adding orange lines for the numbers of combination from the variables
ggplot(tibble(x=1:length(percent),y=cumsum(percent)),aes(x=x,y=y)) +
  geom_line() + scale_y_continuous("variance explained (%)",limits=c(0,100)) +
  scale_x_continuous("Principal component") + 
  geom_vline(xintercept=nvar,colour="red",linetype="dashed",linewidth=0.5) + 
  geom_hline(yintercept=cumsum(percent)[nvar],colour="red",linetype="dashed",linewidth=0.5) +
  geom_vline(xintercept=nlevel,colour="orange",linetype="dashed",linewidth=0.5) + 
  geom_hline(yintercept=cumsum(percent)[nlevel],colour="orange",linetype="dashed",linewidth=0.5)
```

```{r scree-quiz}
quiz(caption="What do we see?",
     question("How much of the variance is explained by the first two components",
              answer("$~30%$"),
              answer("$>50%$",correct=TRUE),
              answer("$>70%$")),
     question("Could the variance explained be in agreement with our model number of variables and combination thereof?",
              answer("Yes",correct=TRUE),
              answer("No"))
     )
```

---

Let us now visualise the components in two dimensions. We will iterate on the 3 first PCs.

Edoardo: We could maybe skip this part, given that they will only have 1 variable?


```{r twod, exercise=TRUE, exercise.eval=FALSE, exercise.setup="prcomp"}
dev.null <- apply(combn(1:3,2),2,function(co){
  pc.dat <- bind_cols(pc$x[,co],
                      as.data.frame(colData(dds)))
  
  print(ggplot(pc.dat %>% rename_with(.cols=1:2,~c("x","y")),
         aes(x=x,y=y,col=Condition,
             shape=Condition,
             text=SampleID)) + 
    geom_point(size=2) + 
    ggtitle("Principal Component Analysis",subtitle="variance stabilized counts") +
    scale_x_continuous(paste0(colnames(pc.dat)[1], " (", percent[co[1]], "%)")) + 
    scale_y_continuous(paste0(colnames(pc.dat)[2], " (", percent[co[2]], "%)")))
})
```

```{r twod-quiz}
quiz(caption="What can we conclude?",
     textAreaInput("your_conclusions", "")
     )
```

Sometimes on PCA is not linked to the main variables analysed in the design of DESeq2, but to single replicates. Write the code to visualise the replicate instead of one of the other variable for the first and third component.

```{r twod_rep, exercise=TRUE, exercise.eval=FALSE, exercise.setup="prcomp"}

```

```{r twod_rep-hint}
ggplot(DATA,
         aes(x=PCX,y=PCY,
             col=VAR1,
             shape=VAR2,
             text=sample_id)) + 
    geom_point(size=2) + 
    ggtitle("Principal Component Analysis",subtitle="variance stabilized counts")
```

```{r twod_rep-solution}
co <- c(1,3)
pc.dat <- bind_cols(pc$x[,co],
                    as.data.frame(colData(dds)))
ggplot(pc.dat %>% rename_with(.cols=1:2,~c("x","y")),
         aes(x=x,y=y,col=Condition,
             shape=Condition,
             text=SampleID)) + 
    geom_point(size=2) + 
    ggtitle("Principal Component Analysis",subtitle="variance stabilized counts") +
    scale_x_continuous(paste0(colnames(pc.dat)[1], " (", percent[co[1]], "%)")) + 
    scale_y_continuous(paste0(colnames(pc.dat)[2], " (", percent[co[2]], "%)"))
```

#### Heat map

We have now observed that the principal components do explain most of the biological variance. We have now a good summarising idea of the sample properties, bu we certainly also want to have a closer understanding of the expression patterns across the samples. For that, we will generate a heat-map. Heat-maps are computationally expensive to create (as they rely on hierarchical clustering of genes and samples), so many tools in practice limit the number of genes used (the defaults vary between 500 and 1000 for DESeq2 and edgeR). I personally dislike such an approach, as the selection of a subset of gene might very well remove important signal. I am rather in favor of trying to remove genes which signal-to-noise ratio is too low to be informative. There are different ways to do that, but here we just use a naïve approach, where we will keep a gene provided it is expressed a given minimal cutoff, in all replicates of at least one condition.


```{r featSel-setup, include=FALSE}
load(here(deseq2_file))
load(here(vst_file))
dds <- estimateSizeFactors(dds)
```


To do this, you will use the `rangeFeatureSelect() function. It will return two graphics, one in a linear scale, the other in a log scale, showing the number of genes expressed at a specific cutoff in all replicates in at least one condition.

```{r featSel, exercise=TRUE, exercise.eval=TRUE, exercise.setup="featSel-setup"}
conds <- factor(paste(dds$Condition))
#Here, you should insert the number of replicates you have per group, rather than 4, which is the number of replcates I had in the default dataset

sels <- rangeFeatureSelect(counts=vst,conditions=conds, nrep=4)
```

Try to select a cutoff value that will allow you to keep around 20 thousand genes, as that will result in a heatmap that is informative but also reasonably fast to generate.

<!--Even here, the result will change depending on the group. Some groups also had different numbers of replicates, so they changed that value when using rangeFeatureSelect()




```{r featSel-quiz}
quiz(caption="Filtering uninformative (_i.e._ most lowly expressed) genes",
     question("What would seem an ideal signal-to-noise cutoff? In general, we want to keep around 20 thousand genes, as that will result in a reasonably fast generation of the heatmap",
              answer("0",message="This would apply no cutoff and hence be inadequate - non expressed genes would remain."),
              answer("1",correct=TRUE),
              answer("2",message="I would be less stringent"),
              answer("3"),
              incorrect="This is my choice, yours may be different.")
)
```
-->

Now that you have made a choice, change the `vst.cutoff` value in the code below and run it. It will take time, up to a few minutes to complete.

```{r hm, exercise=TRUE, exercise.eval=FALSE, exercise.timelimit=600, exercise.lines=20, exercise.setup="featSel"}
# packages for the heat-map and pearson distance 
suppressPackageStartupMessages({
  library(gplots)
  library(hyperSpec)
})

# graphic palette
hpal <- colorRampPalette(c("blue","white","red"))(100)

# cutoff. You should insert the cutoff you selected to keep around 20 thousand genes in the previous plot
vst.cutoff <- 3

# heatmap
hm <- heatmap.2(t(scale(t(vst[sels[[vst.cutoff+1]],]))),
                distfun=pearson.dist,
                hclustfun=function(X){hclust(X,method="ward.D2")},
                labRow = NA,trace = "none",
                labCol = paste(conds,dds$replicate),
                col=hpal,cexCol=0.7)
```

Finally, interpret the heatmap above.

## Conclusion

Think about what are your conclusions regarding the data, do you think you have an ideal situation? Can you anticipate any problems or issues that could affect the results of the differential expression analysis, given what you have discovered today about the data?

## Session Info
It is good practice and highly recommended to always include the session info in your analyses. This way other can reproduce them with the exact same versions of the packages you used. Use the help to find out how to print the session info.

```{r session-info, exercise=TRUE, exercise.eval=TRUE}

```

```{r session-info-hint}
sessionInfo()
```

## Copyright
This material is provided under the following license:

`CC-BY-NC-SA 4.0:` _Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License_

by **BN Bioinformatics Handelsbolag**
